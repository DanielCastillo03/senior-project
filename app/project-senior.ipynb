{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "enclosed-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torchvision import datasets, transforms, models # add models to the list\n",
    "from torchvision.utils import make_grid\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# ignore harmless warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "uniform-trauma",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform_a = transforms.Compose([\n",
    "        transforms.GaussianBlur((3,3)),     \n",
    "        transforms.RandomHorizontalFlip(),  # reverse 50% of images\n",
    "        transforms.Resize(224),             # resize shortest side to 224 pixels\n",
    "        transforms.CenterCrop(224),         # crop longest side to 224 pixels at center\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_transform_b = transforms.Compose([\n",
    "        transforms.RandomRotation(10),      # rotate +/- 10 degrees\n",
    "        transforms.RandomHorizontalFlip(),  # reverse 50% of images\n",
    "        transforms.Resize(224),             # resize shortest side to 224 pixels\n",
    "        transforms.CenterCrop(224),         # crop longest side to 224 pixels at center\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "train_transform_c = transforms.Compose([\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
    "        transforms.RandomHorizontalFlip(),  # reverse 50% of images\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fatty-accounting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CAT', 'DOG']\n",
      "Training images available: 18743\n",
      "Testing images available:  6251\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "root = '../Data/CATS_DOGS'\n",
    "\n",
    "train_data = datasets.ImageFolder(os.path.join(root, 'train')) #, transform=train_transform)\n",
    "\n",
    "test_data = datasets.ImageFolder(os.path.join(root, 'test'), transform=test_transform)\n",
    "\n",
    "trainset_a, trainset_b, trainset_c = torch.utils.data.random_split(train_data, [6247, 6247, 6249])\n",
    "\n",
    "# we assign the .dataset of our subset to be equal to the subset iself\n",
    "# in other words trainset has the correct images (6247), trainset.dataset references the parent dataset, which is\n",
    "# about 18000 images long. We dont want any reference to the parent data set. I think?\n",
    "# trainset_a.dataset = trainset_a\n",
    "# trainset_b.dataset = trainset_b\n",
    "# trainset_c.dataset = trainset_c\n",
    "\n",
    "# print(len(trainset_a.dataset))\n",
    "# print(len(trainset_b))\n",
    "# print(len(trainset_c))\n",
    "\n",
    "# We Split the train set into 3 subsets, then apply a transformation to each of these subsets.\n",
    "# This will serve as a way to \"simulate\" federated learning (a.k.a multiple users with their own data set and transformation)\n",
    "trainset_a.transform = train_transform_a\n",
    "trainset_b.transform = train_transform_b\n",
    "trainset_c.transform = train_transform_c\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# List where we will store our 3 different subsets\n",
    "list_of_datasets = []\n",
    "\n",
    "# We then append the data sets to the list\n",
    "list_of_datasets.append(trainset_a)\n",
    "list_of_datasets.append(trainset_b)\n",
    "list_of_datasets.append(trainset_c)\n",
    "\n",
    "# Loading our test set\n",
    "test_loader = DataLoader(test_data, batch_size=10, shuffle=True)\n",
    "\n",
    "# Concatinate our list of data sets\n",
    "merged_train_data = ConcatDataset(list_of_datasets)\n",
    "# Then load our data (notice we need to shuffle!)\n",
    "merged_train_loader = DataLoader(merged_train_data, batch_size=10, shuffle=True)\n",
    "# merged_train_set_loader = DataLoader(merged_train_data, batch_size=10, shuffle=True)\n",
    "class_names = train_data.classes\n",
    "\n",
    "print(class_names)\n",
    "print(f'Training images available: {len(merged_train_data)}')\n",
    "print(f'Testing images available:  {len(test_data)}')\n",
    "print(type(test_loader))\n",
    "print(type(merged_train_set_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "rocky-split",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ConcatDataset' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-87e79b81d65a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Grab the first batch of 10 images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmerged_train_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Print the labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConcatDataset' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "# Grab the first batch of 10 images\n",
    "for images,labels in merged_train_loader.dataset:\n",
    "    break\n",
    "\n",
    "# Print the labels\n",
    "print('Label:', labels.numpy())\n",
    "print('Class:', *np.array([class_names[i] for i in labels]))\n",
    "\n",
    "im = make_grid(images, nrow=5)  # the default nrow is 8\n",
    "\n",
    "# Inverse normalize the images\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "    std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")\n",
    "im_inv = inv_normalize(im)\n",
    "\n",
    "# Print the images\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.imshow(np.transpose(im_inv.numpy(), (1, 2, 0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aging-disease",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3, 1)\n",
    "        self.fc1 = nn.Linear(54*54*16, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = X.view(-1, 54*54*16)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = self.fc3(X)\n",
    "        return F.log_softmax(X, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "artistic-mayor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvolutionalNetwork(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=46656, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(101)\n",
    "CNNmodel = ConvolutionalNetwork()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(CNNmodel.parameters(), lr=0.001)\n",
    "CNNmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "correct-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>8}')\n",
    "    print(f'________\\n{sum(params):>8}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "surprised-tourism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     162\n",
      "       6\n",
      "     864\n",
      "      16\n",
      " 5598720\n",
      "     120\n",
      "   10080\n",
      "      84\n",
      "     168\n",
      "       2\n",
      "________\n",
      " 5610222\n"
     ]
    }
   ],
   "source": [
    "count_parameters(CNNmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "flush-three",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  batch:  200 [  2000/8000]  loss: 0.77983063  accuracy:  57.800%\n",
      "epoch:  0  batch:  400 [  4000/8000]  loss: 0.51571345  accuracy:  61.400%\n",
      "epoch:  0  batch:  600 [  6000/8000]  loss: 0.50562775  accuracy:  62.117%\n",
      "epoch:  0  batch:  800 [  8000/8000]  loss: 0.48489887  accuracy:  63.087%\n",
      "epoch:  1  batch:  200 [  2000/8000]  loss: 0.51793706  accuracy:  70.000%\n",
      "epoch:  1  batch:  400 [  4000/8000]  loss: 0.80166948  accuracy:  70.350%\n",
      "epoch:  1  batch:  600 [  6000/8000]  loss: 0.41254324  accuracy:  70.633%\n",
      "epoch:  1  batch:  800 [  8000/8000]  loss: 0.72434211  accuracy:  70.300%\n",
      "epoch:  2  batch:  200 [  2000/8000]  loss: 0.49738583  accuracy:  73.050%\n",
      "epoch:  2  batch:  400 [  4000/8000]  loss: 0.66624522  accuracy:  72.550%\n",
      "epoch:  2  batch:  600 [  6000/8000]  loss: 0.93357354  accuracy:  73.000%\n",
      "epoch:  2  batch:  800 [  8000/8000]  loss: 0.82573682  accuracy:  73.213%\n",
      "\n",
      "Duration: 376 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "max_trn_batch = 800\n",
    "max_tst_batch = 300\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    trn_corr = 0\n",
    "    tst_corr = 0\n",
    "    \n",
    "    # Run the training batches\n",
    "    for b, (X_train, y_train) in enumerate(train_loader):\n",
    "        \n",
    "        # Limit the number of batches\n",
    "        if b == max_trn_batch:\n",
    "            break\n",
    "        b+=1\n",
    "        \n",
    "        # Apply the model\n",
    "        y_pred = CNNmodel(X_train)\n",
    "        loss = criterion(y_pred, y_train)\n",
    " \n",
    "        # Tally the number of correct predictions\n",
    "        predicted = torch.max(y_pred.data, 1)[1]\n",
    "        batch_corr = (predicted == y_train).sum()\n",
    "        trn_corr += batch_corr\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print interim results\n",
    "        if b%200 == 0:\n",
    "            print(f'epoch: {i:2}  batch: {b:4} [{10*b:6}/8000]  loss: {loss.item():10.8f}  \\\n",
    "accuracy: {trn_corr.item()*100/(10*b):7.3f}%')\n",
    "\n",
    "    train_losses.append(loss)\n",
    "    train_correct.append(trn_corr)\n",
    "\n",
    "    # Run the testing batches\n",
    "    with torch.no_grad():\n",
    "        for b, (X_test, y_test) in enumerate(test_loader):\n",
    "            # Limit the number of batches\n",
    "            if b == max_tst_batch:\n",
    "                break\n",
    "\n",
    "            # Apply the model\n",
    "            y_val = CNNmodel(X_test)\n",
    "\n",
    "            # Tally the number of correct predictions\n",
    "            predicted = torch.max(y_val.data, 1)[1] \n",
    "            tst_corr += (predicted == y_test).sum()\n",
    "\n",
    "    loss = criterion(y_val, y_test)\n",
    "    test_losses.append(loss)\n",
    "    test_correct.append(tst_corr)\n",
    "\n",
    "print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-technical",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
